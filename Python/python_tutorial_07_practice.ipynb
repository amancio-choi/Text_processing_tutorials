{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 파일을 분석하여 네트워크용 파일로 만들기\n",
    "- 텍스트 파일에서 네트워크 분석을 위한 데이터 추출(노드(node)와 엣지(edge) 생성)\n",
    "- 동시 출현 단어(word co-occurrence) 네트워크: 한 문서나 문장 내에서 같이 사용되는 단어들의 쌍을 추출하고 빈도를 계산\n",
    "- 영화 대본의 등장 인물간 관계 네트워크: 같은 씬에서 대화를 주고 받는 인물들은 서로 연관성이 있을 것이라는 전제에서 출발\n",
    "- **nltk** 라이브러리를 사용한다. **nltk** 라이브러리는 텍스트 자료를 처리하기 위한 다양한 함수들과 모듈들이 내장되어 있다.\n",
    "- **sklearn** 라이브러리에서 **sklearn.feature_extraction.text** 모듈을 사용한다. 여기에는 텍스트에서 정보를 추출하기 위한 다양한 함수들이 내장되어 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process\n",
    "1. 동시 출현 단어(word co-occurrence) 네트워크: 한 문서나 문장 내에서 같이 사용되는 단어들은 서로 연관성이 있을 것이라는 전제에서 출발\n",
    "   - 문서 혹은 문장에서 같이 사용되는 단어들의 쌍을 추출하고 빈도를 계산 후 2 X 2 차원의 데이터프레임을 만듦\n",
    "   - csv 파일로 내보낸 후 gephi에서 처리\n",
    "   - 데이터프레임을 gephi용 파일로 만든 후 저장, gephi에서 읽어 처리\n",
    "1. 영화 대본의 등장 인물간 관계 네트워크: 같은 씬에서 대화를 주고 받는 인물들은 서로 연관성이 있을 것이라는 전제에서 출발\n",
    "   - 영화 대본을 씬으로 구분 후 각 씬에서 대화를 주고 받는 등장 인물들 이름만 추출\n",
    "   - 각 씬을 하나의 문서 내지 문장으로 보고 여기서 동시 출현 인물,즉 인물들의 쌍과 빈도를 구해 데이터프레임으로 만듦\n",
    "   - csv 파일로 내보낸 후 gephi에서 처리\n",
    "   - 데이터프레임을 gephi용 파일로 만든 후 저장, gephi에서 읽어 처리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 동시 출현 단어(word co-occurrence) 네트워크\n",
    "1. 텍스트(Tesla라는 단어가 들어간 뉴스 기사 제목 100개)를 읽어온다.\n",
    "1. 텍스트에서 불필요한 기호나 숫자 등을 삭제하고 불용어(stopwords)를 제거하는 등의 텍스트 정제 작업을 실시한다.\n",
    "1. 각 문서에 나타나는 단어들의 발생 빈도를 구하여 텍스트를 문서-단어(document-term) 행렬로 만든다.\n",
    "1. 문서-단어(document-term) 행렬을 단어-단어 행렬로 변환하여 두 단어간 동시 발생 빈도를 구한다.\n",
    "1. 단어-단어 행렬을 데이터프레임으로 변환하여 csv 파일로 저장한다. 그리고 이 행렬을 네트워크 분석 도구인 gephi에서 읽을 수 있는 파일로 저장한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 텍스트 파일 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/news_titles_tesla.txt', 'r') as f:\n",
    "    news = f.read().splitlines()\n",
    "\n",
    "print(news[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "english_stopwords = stopwords.words('english')\n",
    "english_stopwords.append('reuters')\n",
    "\n",
    "news_cleaned = []\n",
    "for sent in news:\n",
    "    tokenized_sent = word_tokenize(text=sent)\n",
    "\n",
    "    words = []\n",
    "    for word in tokenized_sent:\n",
    "        wrd_lower = word.lower()\n",
    "        if (wrd_lower.isalpha()) and (wrd_lower not in english_stopwords):\n",
    "            wrd_stem = stemmer.stem(wrd_lower)\n",
    "            words.append(wrd_stem)\n",
    "    \n",
    "    cleaned_title = ' '.join(words)\n",
    "    news_cleaned.append(cleaned_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_cleaned[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 네트워크 노드(node)의 수 줄이기\n",
    "- 동시 출현 단어 네트워크는 각 단어가 노드(node)가 되고, 이 단어들간의 관계(동일 맥락에서 같이 사용된 단어들)를 엣지(edge)로 표현한다. 너무 많은 단어들을 노드로 사용할 경우 네트워크의 생성에 오랜 시간이 걸리고 확인하기도 어려우므로 노드의 수를 제한한다.\n",
    "- 아래 코드는 네트워크 그래프의 노드로 사용하기 위해 전체 단어들 중에서 가장 많이 사용된 상위 50개의 단어만 추출하는 단계이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "all_words = []\n",
    "for title in news_cleaned:\n",
    "    words = title.split(' ')\n",
    "    all_words.extend(words)\n",
    "\n",
    "freqs = Counter(all_words)\n",
    "most_freq_wrds = freqs.most_common(50)\n",
    "target_wrds = []\n",
    "for tup in most_freq_wrds:\n",
    "    target_wrds.append(tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_wrds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 동시 출현 단어(word co-occurrences) 데이터 만들기\n",
    "- **sklearn** 라이브러리 활용\n",
    "- 만약 No module named 'sklearn' 오류가 발생하면 이 라이브러리를 설치한다.\n",
    "- pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(vocabulary=target_wrds)\n",
    "docTerm = cv.fit_transform(news_cleaned)\n",
    "\n",
    "docTerm_co = (docTerm.T * docTerm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df = pd.DataFrame.sparse.from_spmatrix(docTerm_co, index=target_wrds, columns=target_wrds)\n",
    "sp_df.iloc[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docTerm_co.setdiag(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df = pd.DataFrame.sparse.from_spmatrix(docTerm_co, index=target_wrds, columns=target_wrds)\n",
    "sp_df.iloc[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df.to_csv('../result/word_cooccurrence_test_reduced_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "Graph = nx.from_pandas_adjacency(sp_df)\n",
    "# nx.info(Graph)\n",
    "nx.write_gexf(Graph, \"../result/cooccurrence_test_reduced.gexf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 영화 대본의 등장 인물간 관계 네트워크\n",
    "- 영화 Love actually의 각 씬(scene)에서 동시에 등장하는 인물들(즉 서로 같은 공간에 있으면서 대화를 나누는 인물들)은 서로 관련이 있다는 것을 전제\n",
    "- 위에서 설명한 word co-occurrence와 동일한 개념. 여기서는 단어 대신 각 씬에 등장하는 인물 이름간의 관계 즉 인물의 동시 출현(co-occurrence)을 계산\n",
    "- 다음과 같이 단계로 영화의 등장 인물간 관계 네트워크를 그릴 수 있다.\n",
    "1. 영화 대본 파일(Love Actually)을 읽어온다.\n",
    "1. 텍스트에서 불필요한 기호나 숫자 등을 삭제하고 빈 행을 제거하는 등의 텍스트 정제 작업을 실시한다.\n",
    "1. 전체 문서를 씬(scene)별로 나눈다. 이는 씬 단위로 구분하여 같이 등장하는 인물들을 확인해야 하기 때문이다.\n",
    "1. 각 씬에서 등장 인물 이름만 남기고 대사와 지문은 삭제한다. 이로써 등장 인물명을 개별 단어처럼 취급할 수 있다.\n",
    "1. 각 씬에 나타나는 인물들의 등장 빈도를 구하여 씬-인물명(document-term) 행렬로 만든다.\n",
    "1. 씬-인물명(document-term) 행렬을 인물명-인물명 행렬로 변환하여 두 인물간 동시 등장 빈도를 구한다.\n",
    "1. 인물명-인물명 행렬을 데이터프레임으로 변환하여 csv 파일로 저장한다. 그리고 이 행렬을 네트워크 분석 도구인 gephi에서 읽을 수 있는 파일로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/love_actually.txt', 'r') as f:\n",
    "    raw_script = f.readlines()\n",
    "\n",
    "raw_script[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = []\n",
    "for line in raw_script:\n",
    "    line = line.strip()\n",
    "    if line != '':\n",
    "        script.append(line)\n",
    "\n",
    "script[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = script[1:-1]\n",
    "script[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 불필요한 라인들을 모두 삭제했으므로 이제 대본을 씬 별로 분리하기만 하면 된다.\n",
    "- 먼저 씬 구분 라인을 찾아서 해당 위치값을 저장하여 이 위치값으로 각 씬의 시작과 끝을 확인한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "scene_idx = []\n",
    "for i, line in enumerate(script):\n",
    "    if re.search('\\[ Scene #\\d{1,2} \\]', line):\n",
    "        scene_idx.append(i)\n",
    "\n",
    "print(len(scene_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_idx.append(len(script))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scene_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srt_end_pairs = []\n",
    "for id_pair in zip(scene_idx[0:], scene_idx[1:]):\n",
    "    srt = id_pair[0] + 1\n",
    "    end = id_pair[1]\n",
    "    srt_end_pairs.append((srt, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_texts = []\n",
    "for id_pair in srt_end_pairs:\n",
    "    scene = script[id_pair[0]:id_pair[1]]\n",
    "    scene_texts.append(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scene_texts[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대본의 등장 인물과 그 인물을 연기한 배우의 이름이 담긴 데이터 파일을 이용하여 대본에 나타나는 등장 인물 이름에 배우 이름을 넣어준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "char_cast = pd.read_csv('../data/love_actually_cast.csv')\n",
    "char_cast.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(char_cast[char_cast['speaker'].str.contains('Billy')]['actor'].to_list()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_per_scene = []\n",
    "for text in scene_texts:\n",
    "    chars = []\n",
    "    for line in text:\n",
    "        if re.search('^[a-zA-Z]+:', line) is not None:\n",
    "            x = re.search('^[a-zA-Z]+:', line).group()\n",
    "            chars.append(x)\n",
    "    # chars = [re.search('^[a-zA-Z]+:', line).group() for line in text if re.search('^[a-zA-Z]+:', line) is not None]\n",
    "    chars = [re.sub(':', '', char) for char in chars]\n",
    "    char_actor = []\n",
    "    for char in chars:\n",
    "        if char in char_cast['speaker'].to_list():\n",
    "            actor = char_cast[char_cast['speaker'].str.contains(char)]['actor'].to_list()[0]\n",
    "            actor = re.sub(' ', '_', actor)\n",
    "            char_actor.append(char + '(' + actor + ')')\n",
    "        # else:\n",
    "        #     continue\n",
    "    char_actor_ = ' '.join(char_actor)\n",
    "    chars_per_scene.append(char_actor_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = []\n",
    "for line in scene_texts[1]:\n",
    "    if re.search('^[a-zA-Z]+:', line) is not None:\n",
    "        x = re.search('^[a-zA-Z]+:', line).group()\n",
    "        chars.append(x)\n",
    "\n",
    "chars_1 = [re.sub(':', '', char) for char in chars]\n",
    "\n",
    "print(chars_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_1 = []\n",
    "for char in chars:\n",
    "    x = re.sub(':', '', char)\n",
    "    chars_1.append(x)\n",
    "\n",
    "print(chars_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 16행으로 된 위의 코드 블록 중 3-7행도 list comprehension으로 표현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = scene_texts[1]\n",
    "x = [re.search('^[a-zA-Z]+:', line).group() for line in text if re.search('^[a-zA-Z]+:', line) is not None]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chars_per_scene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_per_scene = [scene for scene in chars_per_scene if scene != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_per_scene[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 등장 인물 목록 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = [char.split(' ') for char in chars_per_scene]\n",
    "all_chars_ = []\n",
    "for chars in all_chars:\n",
    "    all_chars_.extend(chars)\n",
    "\n",
    "unique_chars = list(set(all_chars_))\n",
    "print(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del unique_chars[0]\n",
    "print(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "cv = CountVectorizer(lowercase=False, token_pattern='[a-zA-Z]+\\([a-zA-Z_]+\\)+')\n",
    "docTerm = cv.fit_transform(chars_per_scene)\n",
    "\n",
    "docTerm_co = (docTerm.T * docTerm)\n",
    "docTerm_co.setdiag(0)\n",
    "\n",
    "names = cv.get_feature_names_out()\n",
    "\n",
    "df = pd.DataFrame(data=docTerm_co.toarray(), columns=names, index=names)\n",
    "\n",
    "Graph_LA = nx.from_pandas_adjacency(df)\n",
    "nx.write_gexf(Graph_LA, \"../result/chars_interactions_loveActually_test_2.gexf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- chars_interactions_loveActually_test_2.gexf 파일을 gephi로 열어서 네트워크를 그리면 아래 그림과 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2924a307e1aaa532cc473f8009b33dacafd8f63580212621cca8fcd8b3d0ec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
