{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트에서 단어 목록 추출하기 및 본문에서 단어 키워드 검색하기\n",
    "- 분석 대상 문서 파일 읽기\n",
    "- 문서 정제 실시\n",
    "- 단어 목록 생성 및 파일 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process\n",
    "1. 텍스트 파일(.txt)이나 엑셀, csv 등의 데이터베이스 파일을 읽는다.\n",
    "1. 문서 정제 작업\n",
    "   - Clean texts: 텍스트에서 불필요한 부분 삭제(예: 분석 대상인 본문 이외의 header, footer 등, 구두점, 기호 등)\n",
    "   - Normalize texts: 텍스트에 나타나는 변이형들의 통일(예: 대소문자 통일, 스펠링 차이 통일 등)\n",
    "   - Tag texts: 단어의 품사 정보 부착(품사 구분이 필요한 경우에 한해)\n",
    "   - Split texts: 문서를 문장으로, 문장을 단어로 분리\n",
    "2. 단어 분리 및 갯수 세기 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 텍스트의 단어 목록과 빈도 정보 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_sample = \"To Sherlock Holmes she is always the woman. I have seldom heard him mention her under any other name. \\\n",
    "In his eyes she eclipses and predominates the whole of her sex. It was not that he felt any emotion akin to love for Irene Adler. \\\n",
    "All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. \\\n",
    "He was, I take it, the most perfect reasoning and observing machine that the world has seen, \\\n",
    "but as a lover he would have placed himself in a false position. He never spoke of the softer passions, save with a gibe and a sneer. \\\n",
    "They were admirable things for the observer—excellent for drawing the veil from men’s motives and actions. \\\n",
    "But for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce \\\n",
    "a distracting factor which might throw a doubt upon all his mental results. Grit in a sensitive instrument, \\\n",
    "or a crack in one of his own high-power lenses, would not be more disturbing than a strong emotion in a nature such as his. \\\n",
    "And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_cleaned = txt_sample.replace('.', '').replace(',', '')\n",
    "txt_splitted = txt_cleaned.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt_cleaned); print(txt_splitted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어로 분리된 믄자열 리스트에서 특정 단어의 사용 횟수, 즉 빈도를 확인할 때에는 **count()** 함수를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt_splitted.count('a')); print(txt_splitted.count('He'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 함수들을 이용하여 *txt_sample*에 들어있는 모든 단어들의 빈도를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_cleaned = txt_sample.replace('.', '').replace(',', '')\n",
    "txt_splitted = txt_cleaned.split(' ')\n",
    "unique_words = set(txt_splitted)\n",
    "# print(unique_words)\n",
    "\n",
    "for word in list(unique_words)[100:115]:\n",
    "    print('Frequency of', word , ':', txt_splitted.count(word))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그러나 위 출력 결과에서 보듯이 대문자로 시작하는 단어와 소문자로 시작하는 단어가 섞여 있으며 이들은 별개의 다른 단어로 저장되어 있다. 아래에서 'And'와 'and' 등 2개가 다 저장되어 있는지 확인해 보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('And' in list(unique_words)); print('and' in list(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_cleaned = txt_sample.replace('.', '').replace(',', '')\n",
    "txt_normed = txt_cleaned.lower()\n",
    "txt_splitted = txt_normed.split(' ')\n",
    "unique_words = set(txt_splitted)\n",
    "\n",
    "for word in list(unique_words)[:10]:\n",
    "    print('Frequency of', word , ':', txt_splitted.count(word))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 단어 목록과 빈도를 구한 후 빈도 목록 데이터프레임 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_cleaned = txt_sample.replace('.', '').replace(',', '')\n",
    "txt_normed = txt_cleaned.lower()\n",
    "txt_splitted = txt_normed.split(' ')\n",
    "unique_words = set(txt_splitted)\n",
    "\n",
    "freqs = {}\n",
    "for word in list(unique_words):\n",
    "    freqs[word] = txt_splitted.count(word)\n",
    "\n",
    "print(freqs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 빈도 정보는 **collections** 라이브러리의 **Counter** 모듈을 이용하여 구할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "txt_cleaned = txt_sample.replace('.', '').replace(',', '')\n",
    "txt_normed = txt_cleaned.lower()\n",
    "txt_splitted = txt_normed.split(' ')\n",
    "\n",
    "freqs = Counter(txt_splitted)\n",
    "print(freqs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 딕셔너리로 저장된 빈도 정보를 데이터프레임으로 변환하여 엑셀 파일로 내보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freqs.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "freq_df = pd.DataFrame(freqs.items(), columns=['word', 'count'])\n",
    "freq_df = freq_df.sort_values(by=['count'], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([freq_df.head(), freq_df.tail()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. \\<The Adventure of Sherlock Holmes\\> 데이터 읽어와서 내용 살펴보기(https://www.gutenberg.org/cache/epub/1661/pg1661-images.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반 텍스트 파일(.txt)을 읽어와서 단어 빈도 목록 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file='../data/sherlock_holmes_doyle.txt', mode='r') as f:\n",
    "    advan_holmes = f.read().replace('\\ufeff', '').strip().splitlines()\n",
    "\n",
    "advan_holmes = advan_holmes[53:11951]\n",
    "advan_holmes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "words_list = []\n",
    "for line in advan_holmes:\n",
    "    if line != '':\n",
    "        line_lower = line.lower()\n",
    "        line_lower = re.sub(pattern='[\\.\\!\\?,\\'\\\"“”]', repl='', string=line_lower)\n",
    "        line_splitted = line_lower.split(' ')\n",
    "        words_list.extend(line_splitted)\n",
    "\n",
    "words_freqs = Counter(words_list)\n",
    "\n",
    "freq_df = pd.DataFrame(words_freqs.items(), columns=['word', 'count'])\n",
    "freq_df = freq_df.sort_values(by=['count'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "freq_df.to_excel('../result/word_freq_sherlock_holmes.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **append()**와 **extend()**의 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['Google', 'Facebook', 'Twitter']\n",
    "y = ['Instagram', 'TikTok']\n",
    "x.append(y)\n",
    "print('Appended x:', x)\n",
    "\n",
    "x = ['Google', 'Facebook', 'Twitter']\n",
    "y = ['Instagram', 'TikTok']\n",
    "x.extend(y)\n",
    "print('Extended x:', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 데이터프레임 형식으로 저장된 csv 파일을 읽어와서 단어 빈도 목록 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sherlock_df = pd.read_csv('../result/sherlock_holmes_updated.csv')\n",
    "sherlock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sherlock_body = sherlock_df['body'].to_list()\n",
    "len(sherlock_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_lists = []\n",
    "for body in sherlock_body:\n",
    "    bdy = body.lower()\n",
    "    bdy = re.sub(pattern='[\\.\\!\\?,\\'\\\"“”]', repl='', string=bdy)\n",
    "    # bdy_splitted = bdy.split(' ')\n",
    "    bdy_splitted = re.split(' |<p>', bdy)\n",
    "    word_lists.extend(bdy_splitted)\n",
    "\n",
    "words_freqs = Counter(word_lists)\n",
    "\n",
    "for itm in list(words_freqs.items())[:10]:\n",
    "    print(itm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_per_chapter = []\n",
    "for i, row in sherlock_df.iterrows():\n",
    "    bdy = row['body'].lower()\n",
    "    bdy = re.sub(pattern='[\\.\\!\\?,\\'\\\"“”]', repl='', string=bdy)\n",
    "    # bdy_splitted = bdy.split(' ')\n",
    "    bdy_splitted = re.split(' |<p>', bdy)\n",
    "    freq = Counter(bdy_splitted)\n",
    "    freqs_per_chapter.append(freq)\n",
    "\n",
    "len(freqs_per_chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(freqs_per_chapter[0].items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10 (default, Feb 26 2021, 18:47:35) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2924a307e1aaa532cc473f8009b33dacafd8f63580212621cca8fcd8b3d0ec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
